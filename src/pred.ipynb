{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "MODEL_NAME = \"gpt-4.1-mini\"\n",
    "\n",
    "MAX_CHARS_PER_CASE = 50000        # truncate full case text\n",
    "MAX_CHARS_PER_SNIPPET = 12000     # truncate row text (snippet)\n",
    "DELAY_BETWEEN_CALLS_SEC = 1.2     # anti rate-limit cushion\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "RETRY_BASE_SLEEP = 1.5\n",
    "RETRY_JITTER_MAX = 0.5\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "os.environ[\"INPUT_CSV\"] \n",
    "os.environ[\"OUTPUT_DIR\"]\n",
    "os.environ[\"PROMPTS_YAML\"]\n",
    "\n",
    "input_csv = Path(os.environ[\"INPUT_CSV\"])\n",
    "output_dir = Path(os.environ[\"OUTPUT_DIR\"])\n",
    "\n",
    "\n",
    "# if you want logs\n",
    "LOGS_DIR = output_dir / \"logs\"\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# API key (prefer env var)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "PROMPTS_YAML = Path(os.environ[\"PROMPTS_YAML\"])\n",
    "\n",
    "_PROMPTS_CACHE: Dict[str, Any] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e75bf49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def load_prompts_yaml(path: Path = PROMPTS_YAML) -> Dict[str, Any]:\n",
    "    global _PROMPTS_CACHE\n",
    "    if _PROMPTS_CACHE:\n",
    "        return _PROMPTS_CACHE\n",
    "    data = yaml.safe_load(path.read_text(encoding=\"utf-8\"))\n",
    "    if not isinstance(data, dict):\n",
    "        raise ValueError(\"prompts.yaml must be a dict at top level\")\n",
    "    _PROMPTS_CACHE = data\n",
    "    return _PROMPTS_CACHE\n",
    "\n",
    "def _template_fields(tpl: str) -> set:\n",
    "    return {fname for _, fname, _, _ in string.Formatter().parse(tpl) if fname}\n",
    "\n",
    "def norm_text(x) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, float) and math.isnan(x):\n",
    "        return \"\"\n",
    "    return str(x)\n",
    "\n",
    "def safe_int(x, default=0) -> int:\n",
    "    try:\n",
    "        if x is None:\n",
    "            return default\n",
    "        if isinstance(x, float) and math.isnan(x):\n",
    "            return default\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def build_case_fulltext_map(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    case_file_name -> concatenation of all 'text' rows for that case\n",
    "    \"\"\"\n",
    "    m: Dict[str, str] = {}\n",
    "    for case, g in df.groupby(\"case_file_name\", sort=False):\n",
    "        parts = [norm_text(t).strip() for t in g[\"text\"].tolist()]\n",
    "        parts = [p for p in parts if p]\n",
    "        full = \"\\n\".join(parts)\n",
    "        # normalize + truncate\n",
    "        full = \"\\n\".join([ln.strip() for ln in full.splitlines() if ln.strip()])\n",
    "        m[str(case)] = full[:MAX_CHARS_PER_CASE]\n",
    "    return m\n",
    "\n",
    "def build_prompt_defendants(\n",
    "    case_file_name: str,\n",
    "    num_defendants: int,\n",
    "    full_case_text: str,\n",
    "    snippet_text: str,\n",
    ") -> str:\n",
    "    snippet_text = (snippet_text or \"\")[:MAX_CHARS_PER_SNIPPET]\n",
    "    full_case_text = (full_case_text or \"\")[:MAX_CHARS_PER_CASE]\n",
    "\n",
    "    prompts = load_prompts_yaml()\n",
    "    cfg = prompts[\"defendants_labeling\"]          # key in yaml\n",
    "    tpl = cfg[\"user_template\"]                    # the prompt template string\n",
    "\n",
    "    return tpl.format(\n",
    "        case_file_name=case_file_name,\n",
    "        num_defendants=num_defendants,\n",
    "        full_case_text=full_case_text,\n",
    "        snippet_text=snippet_text,\n",
    "    )\n",
    "\n",
    "def parse_defendants_digits(output: str, num_defendants: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Expect \"0\" or a digit string like \"23\" (for N<=9).\n",
    "    Still robust if model returns \"2 3\" / \"2,3\" / \"2-3\" etc.\n",
    "    \"\"\"\n",
    "    out = (output or \"\").strip()\n",
    "\n",
    "    if not out:\n",
    "        return []\n",
    "\n",
    "    if out == \"0\":\n",
    "        return []\n",
    "\n",
    "    if out.isdigit():\n",
    "        if num_defendants <= 9:\n",
    "            defs = [int(ch) for ch in out]\n",
    "        else:\n",
    "            # N>9 is ambiguous in the \"23\" encoding; keep as one int (rare)\n",
    "            defs = [int(out)]\n",
    "    else:\n",
    "        import re\n",
    "        defs = [int(x) for x in re.findall(r\"\\d+\", out)]\n",
    "\n",
    "    # keep only 1..N, unique, sorted\n",
    "    defs = sorted({d for d in defs if 1 <= d <= num_defendants})\n",
    "    return defs\n",
    "def format_defendants_str(defs: List[int]) -> str:\n",
    "    # If none -> \"0\"\n",
    "    if not defs:\n",
    "        return \"0\"\n",
    "    if any(d >= 10 for d in defs):\n",
    "        return \",\".join(str(d) for d in defs)\n",
    "    return \"\".join(str(d) for d in defs)\n",
    "\n",
    "\n",
    "def call_llm_with_retry(prompt: str) -> str:\n",
    "    last_err: Optional[Exception] = None\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You output ONLY digits. If none, output 0.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                max_tokens=8,\n",
    "            )\n",
    "            return (resp.choices[0].message.content or \"\").strip()\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            sleep_s = RETRY_BASE_SLEEP * (2 ** (attempt - 1))\n",
    "            sleep_s += random.random() * RETRY_JITTER_MAX\n",
    "            time.sleep(sleep_s)\n",
    "    raise last_err if last_err else RuntimeError(\"Unknown LLM error\")\n",
    "\n",
    "\n",
    "def build_local_context_map(df: pd.DataFrame, k: int = 3) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Returns: row_index -> local_context (k rows before + current + k rows after) within same case_file_name\n",
    "    Assumes df is in the same order you want (donâ€™t sort unless you mean to).\n",
    "    \"\"\"\n",
    "    ctx: Dict[int, str] = {}\n",
    "\n",
    "    # keep original row indices\n",
    "    for case, g in df.groupby(\"case_file_name\", sort=False):\n",
    "        idxs = g.index.tolist()\n",
    "        texts = [norm_text(t).strip() for t in g[\"text\"].tolist()]\n",
    "\n",
    "        for pos, row_idx in enumerate(idxs):\n",
    "            lo = max(0, pos - k)\n",
    "            hi = min(len(idxs), pos + k + 1)\n",
    "\n",
    "            lines = []\n",
    "            for j in range(lo, hi):\n",
    "                tag = \"CURRENT\" if j == pos else (\"PREV\" if j < pos else \"NEXT\")\n",
    "                lines.append(f\"{tag}: {texts[j]}\")\n",
    "\n",
    "            local = \"\\n\".join([ln for ln in lines if ln.strip()])\n",
    "            ctx[row_idx] = local[:MAX_CHARS_PER_SNIPPET]  # keep it bounded\n",
    "    return ctx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "70ff075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_defendants_labeling(\n",
    "    input_csv: Path,\n",
    "    output_dir: Path,\n",
    ") -> Path:\n",
    "    ts = dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_csv = output_dir / f\"defendants_pred_{ts}.csv\"\n",
    "    log_path = LOGS_DIR / f\"run_defendants_{ts}.log\"\n",
    "\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # make sure pred exists (we do NOT touch defendants_str - it's the label)\n",
    "    if \"pred\" not in df.columns:\n",
    "        df[\"pred\"] = \"\"\n",
    "\n",
    "    required = {\"case_file_name\", \"text\", \"defendants_str\", \"pred\", \"num_defendants\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    # enforce output schema\n",
    "    df = df[[\"case_file_name\", \"text\", \"defendants_str\", \"pred\", \"num_defendants\"]]\n",
    "\n",
    "    case_map = build_case_fulltext_map(df)\n",
    "\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as log_f:\n",
    "        total = len(df)\n",
    "        log_f.write(f\"[{ts}] Start defendants prediction: {total} rows\\n\")\n",
    "        log_f.flush()\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            case = str(row.get(\"case_file_name\", \"\")).strip()\n",
    "            n = safe_int(row.get(\"num_defendants\"), 0)\n",
    "            snippet = norm_text(row.get(\"text\", \"\")).strip()\n",
    "\n",
    "            # always predict for ALL rows\n",
    "            if n <= 0 or not snippet:\n",
    "                df.at[idx, \"pred\"] = \"0\"\n",
    "                log_f.write(f\"[{idx+1}/{total}] case={case} -> pred='0' (empty text or num_defendants={row.get('num_defendants')})\\n\")\n",
    "                log_f.flush()\n",
    "                continue\n",
    "\n",
    "            full_case_text = case_map.get(case, \"\")\n",
    "            prompt = build_prompt_defendants(case, n, full_case_text, snippet)\n",
    "\n",
    "            try:\n",
    "                raw = call_llm_with_retry(prompt)   # should be digits / 0\n",
    "                df.at[idx, \"pred\"] = raw            # keep prediction only in pred\n",
    "\n",
    "                # (optional) normalize outputs like \"0.0\" -> \"0\"\n",
    "                s = norm_text(raw).strip()\n",
    "                if s.endswith(\".0\"):\n",
    "                    s = s[:-2]\n",
    "                    df.at[idx, \"pred\"] = s\n",
    "\n",
    "                log_f.write(f\"[{idx+1}/{total}] case={case} -> pred='{df.at[idx,'pred']}'\\n\")\n",
    "                log_f.flush()\n",
    "                time.sleep(DELAY_BETWEEN_CALLS_SEC)\n",
    "\n",
    "            except Exception as e:\n",
    "                df.at[idx, \"pred\"] = \"0\"\n",
    "                log_f.write(f\"[{idx+1}/{total}] case={case} -> ERROR: {e} (set pred=0)\\n\")\n",
    "                log_f.flush()\n",
    "\n",
    "        log_f.write(f\"[{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}] Finished.\\n\")\n",
    "        log_f.flush()\n",
    "\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"âœ… saved: {out_csv}\")\n",
    "    print(f\"ðŸ—’ï¸  log:   {log_path}\")\n",
    "    return out_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_defendants_labeling(\n",
    "    input_csv=input_csv,\n",
    "    output_dir=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6660f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Union, Set, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def evaluate_defendants_csv(csv_path: Union[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input CSV columns:\n",
    "      case_file_name,text,defendants_str,pred,num_defendants\n",
    "\n",
    "    defendants_str = true value (\"0\" means none)\n",
    "    pred           = predicted value (\"0\" means none)\n",
    "\n",
    "    Returns a df with rows: overall, N=2, N=3\n",
    "    Columns: micro_precision, micro_recall, micro_f1, avg_jaccard, accuracy\n",
    "    (accuracy here = exact set match, per-row)\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    required = {\"defendants_str\", \"pred\", \"num_defendants\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    def _to_int(x, default=0) -> int:\n",
    "        try:\n",
    "            if x is None:\n",
    "                return default\n",
    "            if isinstance(x, float) and np.isnan(x):\n",
    "                return default\n",
    "            return int(float(x))\n",
    "        except Exception:\n",
    "            return default\n",
    "\n",
    "    def _clean_str(x: Any) -> str:\n",
    "        if x is None:\n",
    "            return \"0\"\n",
    "        if isinstance(x, float) and np.isnan(x):\n",
    "            return \"0\"\n",
    "        s = str(x).strip()\n",
    "        if s == \"\":\n",
    "            return \"0\"\n",
    "        # normalize \"23.0\" -> \"23\"\n",
    "        if re.fullmatch(r\"\\d+\\.0\", s):\n",
    "            s = s[:-2]\n",
    "        return s\n",
    "\n",
    "    def _parse_set(val: Any, n: int) -> Set[int]:\n",
    "        s = _clean_str(val)\n",
    "        if s == \"0\":\n",
    "            return set()\n",
    "\n",
    "        if not s.isdigit():\n",
    "            nums = [int(x) for x in re.findall(r\"\\d+\", s)]\n",
    "            return {d for d in nums if 1 <= d <= n}\n",
    "\n",
    "        if n <= 9:\n",
    "            return {int(ch) for ch in s if ch.isdigit() and 1 <= int(ch) <= n}\n",
    "\n",
    "        nums = [int(x) for x in re.findall(r\"\\d+\", s)]\n",
    "        return {d for d in nums if 1 <= d <= n}\n",
    "\n",
    "    def _score(sub: pd.DataFrame) -> dict:\n",
    "        TP = FP = FN = 0\n",
    "        jaccs = []\n",
    "        exact_matches = 0\n",
    "        counted = 0\n",
    "\n",
    "        for _, r in sub.iterrows():\n",
    "            n = _to_int(r[\"num_defendants\"], 0)\n",
    "            if n <= 0:\n",
    "                continue\n",
    "\n",
    "            t = _parse_set(r[\"defendants_str\"], n)\n",
    "            p = _parse_set(r[\"pred\"], n)\n",
    "\n",
    "            TP += len(t & p)\n",
    "            FP += len(p - t)\n",
    "            FN += len(t - p)\n",
    "\n",
    "            union = len(t | p)\n",
    "            jaccs.append(1.0 if union == 0 else len(t & p) / union)\n",
    "\n",
    "            exact_matches += int(t == p)\n",
    "            counted += 1\n",
    "\n",
    "        prec = TP / (TP + FP) if (TP + FP) else 1.0\n",
    "        rec  = TP / (TP + FN) if (TP + FN) else 1.0\n",
    "        f1   = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n",
    "        avg_j = float(np.mean(jaccs)) if jaccs else 0.0\n",
    "        acc = (exact_matches / counted) if counted else 0.0\n",
    "\n",
    "        return {\n",
    "            \"micro_precision\": prec,\n",
    "            \"micro_recall\": rec,\n",
    "            \"micro_f1\": f1,\n",
    "            \"accuracy\": acc,\n",
    "            \"avg_jaccard\": avg_j\n",
    "        }\n",
    "\n",
    "    df[\"num_defendants_int\"] = df[\"num_defendants\"].apply(_to_int)\n",
    "\n",
    "    overall = _score(df)\n",
    "    n2 = _score(df[df[\"num_defendants_int\"] == 2])\n",
    "    n3 = _score(df[df[\"num_defendants_int\"] == 3])\n",
    "\n",
    "    out = pd.DataFrame([overall, n2, n3], index=[\"overall\", \"N=2\", \"N=3\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec2490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         micro_precision  micro_recall  micro_f1  accuracy  avg_jaccard\n",
      "overall         0.749917      0.787478  0.768239  0.719039     0.739921\n",
      "N=2             0.812620      0.826848  0.819672  0.761682     0.781153\n",
      "N=3             0.714837      0.761010  0.737201  0.690094     0.710293\n"
     ]
    }
   ],
   "source": [
    "file  = \"\"\n",
    "eval_df = evaluate_defendants_csv(file)\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = output_dir / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fn = results_dir / f\"eval_defendants_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "eval_df.to_csv(fn, index=True, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Saved evaluation CSV: {fn}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
